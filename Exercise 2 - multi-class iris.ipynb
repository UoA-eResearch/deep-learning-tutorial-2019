{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise 2 - multi-class iris.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UoA-eResearch/deep-learning-tutorial-2019/blob/master/Exercise%202%20-%20multi-class%20iris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPQTvjnTzTqM",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2: The Iris Dataset\n",
        "In this exercise we will create a neural network to classify 3 different types of Iris (Setosa, Versicolor and Virginica) based on their sepal length, sepal width, petal length and petal width.\n",
        "\n",
        "![Irises](http://dataaspirant.com/wp-content/uploads/2017/01/irises.png)\n",
        "\n",
        "This is a multi class classification problem. It is similar to the Pima Indian's binary classification exercise, but with three classes to predict instead of two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x9VrMNin1Yi",
        "colab_type": "text"
      },
      "source": [
        "### Q: How many steps are there in creating a neural network model? Please list those steps\n",
        "\n",
        "*answer...*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6B9pUqCzTqO",
        "colab_type": "text"
      },
      "source": [
        "### Import dependencies\n",
        "Start by importing the dependencies we will need for the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB8CJHppzTqP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6afb6fb-1471-4aad-f839-dbfe93dae12d"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxNqXnJTzTqY",
        "colab_type": "text"
      },
      "source": [
        "### Import data\n",
        "The Iris dataset contains four features from 150 different Iris flowers. The features in the dataset are described below.\n",
        "\n",
        "* Sepal length (cm)\n",
        "* Sepal width (cm)\n",
        "* Petal length (cm)\n",
        "* Petal width (cm)\n",
        "* Class: Iris setosa, Iris versicolor or Iris virginica\n",
        "\n",
        "Sepals are the part of a flower that protect and support the petals. The petals surround the reproductive parts of the flower.\n",
        "\n",
        "![Iris labeled](http://terpconnect.umd.edu/~petersd/666/html/iris_with_labels.jpg)\n",
        "\n",
        "A snapshot of the dataset is illustrated below (not in order).\n",
        "\n",
        "|Sepal Length|Sepal Width|Petal Length|Petal Width|Class|\n",
        "|---|---|---|---|-----------|\n",
        "|5.1|3.5|1.4|0.2|Iris-setosa|\n",
        "|4.9|3.0|1.4|0.2|Iris-setosa|\n",
        "|7.0|3.2|4.7|1.4|Iris-versicolor|\n",
        "|6.4|3.2|4.5|1.5|Iris-versicolor|\n",
        "|6.3|3.3|6.0|2.5|Iris-virginica|\n",
        "|5.8|2.7|5.1|1.9|Iris-virginica|\n",
        "\n",
        "To load this data into memory, use the `np.loadtxt` function. The data type (`dtype`) is set to `str` because our input data is a mix of numbers and strings. This will be dealt with when we split the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wYz6SEDzTqY",
        "colab_type": "code",
        "outputId": "239d03e2-bba5-455f-dcd3-de794f736744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "data = np.loadtxt('https://raw.githubusercontent.com/UoA-eResearch/deep-learning-tutorial-2019/master/data/iris.csv', delimiter=\",\", dtype=str)\n",
        "print(data[:6]) #Show the first 6 rows"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['5.1' '3.5' '1.4' '0.2' 'Iris-setosa']\n",
            " ['4.9' '3.0' '1.4' '0.2' 'Iris-setosa']\n",
            " ['4.7' '3.2' '1.3' '0.2' 'Iris-setosa']\n",
            " ['4.6' '3.1' '1.5' '0.2' 'Iris-setosa']\n",
            " ['5.0' '3.6' '1.4' '0.2' 'Iris-setosa']\n",
            " ['5.4' '3.9' '1.7' '0.4' 'Iris-setosa']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGLO13AQzTqb",
        "colab_type": "text"
      },
      "source": [
        "Separate the data into input (X) and output (y) variables.\n",
        "\n",
        "Note that we convert the input data into floats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-EcR4awzTqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data[:, 0:4].astype(float)\n",
        "y = data[:, 4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2zYDMAbzTqd",
        "colab_type": "text"
      },
      "source": [
        "If you look carefully at the target values, you will notice that they are strings, i.e. 'Iris-setosa', 'Iris-versicolor' and 'Iris-virginica'.\n",
        "\n",
        "**Keras needs numbers or matrices to work with, so we will need to reformat the target values.**\n",
        "\n",
        "The problem with converting the class values to numbers (e.g. 'Iris-setosa' becomes 0, 'Iris-versicolor' 1 etc) is that it implies that the target values are ordinal. That is, 'Iris-setosa' is somehow less than 'Iris-versicolor', which is not the case for this dataset.\n",
        "\n",
        "A better way to represent classes in a multi-class classification problem, is to 'one hot encode' the target values. An example is shown below. A matrix of zeros is generated. Each row corresponds to a sample and each column corresponds to a particular class. A 1 is placed into the column to incidicate the class that it belongs too.\n",
        "\n",
        "|Iris-setosa|Iris-versicolor|Iris-virginica|\n",
        "|---|---|---|\n",
        "|1|0|0|\n",
        "|0|1|0|\n",
        "|0|0|1|\n",
        "\n",
        "One hot encoding is a two step process. First encode the target values (y) into an array of numbers using the `LabelEncoder` from scikit-learn and then one hot encode the numbers with the `np_utils.to_categorical` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmReXRywzTqe",
        "colab_type": "code",
        "outputId": "1668fd63-13d0-4394-e49a-9f4de179db9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_encoded = LabelEncoder().fit(y).transform(y) # Convert the classes into numbers\n",
        "y_encoded[45:55]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQgfvZcfzTqi",
        "colab_type": "code",
        "outputId": "f2969c75-8f30-4673-eaa8-ad2762b6e084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "y_one_hot_encoded = np_utils.to_categorical(y_encoded) # One hot encode the numbers\n",
        "y_one_hot_encoded[45:55]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMbd6-LV9Rm_",
        "colab_type": "text"
      },
      "source": [
        "### Set seed\n",
        "Set a seed value so that when we repeatedly run our code we will get the same result. Using the same seed is important when you want to compare algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvjmpbYM9UN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 7\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9s47Jx9zTqk",
        "colab_type": "text"
      },
      "source": [
        "Like the previous exercise, use the `train_test_split` function from scikit-learn to split the input and target data into training, validation and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qdeRm6LzTql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot_encoded, test_size=0.2, random_state=seed)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gp1FcnQzTqn",
        "colab_type": "text"
      },
      "source": [
        "### Create the model\n",
        "The code snippet below creates a very basic neural network model, with three layers: an input layer, a hidden layer and an output layer.\n",
        "\n",
        "The first layer is a fully connected `Dense` layer. We use four neurons in the hidden layer and have 4 input neurons for the 4 features.\n",
        "\n",
        "The last layer has 3 neurons, one for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG9oXx6dzTqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "0dafcafa-25fa-437d-b23c-049eaf264a52"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(4, input_dim=4, activation='relu', kernel_initializer='normal'))\n",
        "model.add(Dense(3, activation='sigmoid', kernel_initializer='normal'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0710 05:27:37.021554 139944564778880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0710 05:27:37.074593 139944564778880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0710 05:27:37.082700 139944564778880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McwgpeyfzTqp",
        "colab_type": "text"
      },
      "source": [
        "### Compile the model\n",
        "We then compile the model. The loss function is set to `categorical_crossentropy` (different from the loss function used in the binary classification exercise) because we are performing multi-class classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F86R20yJzTqp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ea26ac91-0d0b-4951-c7cf-0216aac5a33d"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0710 05:27:37.139530 139944564778880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0710 05:27:37.176618 139944564778880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr-3q6CEzTqs",
        "colab_type": "text"
      },
      "source": [
        "### Fit the model\n",
        "Now that we have compiled the model, we can train it with the data we prepared earlier. We are using more epochs but a smaller batch size than the previous exercise.\n",
        "\n",
        "To see the model training history in text, just don't include `verbose=0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0ohIS-DzTqs",
        "colab_type": "code",
        "outputId": "771eed57-0aeb-4cd3-f910-4b218e26e66e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, batch_size=5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0710 05:27:37.340507 139944564778880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0710 05:27:37.393544 139944564778880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 96 samples, validate on 24 samples\n",
            "Epoch 1/200\n",
            "96/96 [==============================] - 1s 8ms/step - loss: 1.0998 - acc: 0.3438 - val_loss: 1.0979 - val_acc: 0.4167\n",
            "Epoch 2/200\n",
            "96/96 [==============================] - 0s 293us/step - loss: 1.0972 - acc: 0.5833 - val_loss: 1.0968 - val_acc: 0.6667\n",
            "Epoch 3/200\n",
            "96/96 [==============================] - 0s 312us/step - loss: 1.0953 - acc: 0.6458 - val_loss: 1.0951 - val_acc: 0.6250\n",
            "Epoch 4/200\n",
            "96/96 [==============================] - 0s 277us/step - loss: 1.0926 - acc: 0.3854 - val_loss: 1.0932 - val_acc: 0.2500\n",
            "Epoch 5/200\n",
            "96/96 [==============================] - 0s 274us/step - loss: 1.0887 - acc: 0.3542 - val_loss: 1.0899 - val_acc: 0.2500\n",
            "Epoch 6/200\n",
            "96/96 [==============================] - 0s 279us/step - loss: 1.0836 - acc: 0.3438 - val_loss: 1.0855 - val_acc: 0.2500\n",
            "Epoch 7/200\n",
            "96/96 [==============================] - 0s 294us/step - loss: 1.0770 - acc: 0.3854 - val_loss: 1.0792 - val_acc: 0.4167\n",
            "Epoch 8/200\n",
            "96/96 [==============================] - 0s 305us/step - loss: 1.0689 - acc: 0.5625 - val_loss: 1.0707 - val_acc: 0.6250\n",
            "Epoch 9/200\n",
            "96/96 [==============================] - 0s 277us/step - loss: 1.0586 - acc: 0.6458 - val_loss: 1.0612 - val_acc: 0.6667\n",
            "Epoch 10/200\n",
            "96/96 [==============================] - 0s 277us/step - loss: 1.0470 - acc: 0.6563 - val_loss: 1.0499 - val_acc: 0.6667\n",
            "Epoch 11/200\n",
            "96/96 [==============================] - 0s 319us/step - loss: 1.0336 - acc: 0.6875 - val_loss: 1.0356 - val_acc: 0.6667\n",
            "Epoch 12/200\n",
            "96/96 [==============================] - 0s 314us/step - loss: 1.0180 - acc: 0.6875 - val_loss: 1.0202 - val_acc: 0.6667\n",
            "Epoch 13/200\n",
            "96/96 [==============================] - 0s 290us/step - loss: 1.0001 - acc: 0.6875 - val_loss: 1.0003 - val_acc: 0.6667\n",
            "Epoch 14/200\n",
            "96/96 [==============================] - 0s 323us/step - loss: 0.9805 - acc: 0.6875 - val_loss: 0.9814 - val_acc: 0.6667\n",
            "Epoch 15/200\n",
            "96/96 [==============================] - 0s 304us/step - loss: 0.9589 - acc: 0.6875 - val_loss: 0.9599 - val_acc: 0.6667\n",
            "Epoch 16/200\n",
            "96/96 [==============================] - 0s 267us/step - loss: 0.9378 - acc: 0.6875 - val_loss: 0.9368 - val_acc: 0.6667\n",
            "Epoch 17/200\n",
            "96/96 [==============================] - 0s 300us/step - loss: 0.9171 - acc: 0.6875 - val_loss: 0.9171 - val_acc: 0.6667\n",
            "Epoch 18/200\n",
            "96/96 [==============================] - 0s 274us/step - loss: 0.8927 - acc: 0.6875 - val_loss: 0.8919 - val_acc: 0.6667\n",
            "Epoch 19/200\n",
            "96/96 [==============================] - 0s 289us/step - loss: 0.8703 - acc: 0.6875 - val_loss: 0.8680 - val_acc: 0.6667\n",
            "Epoch 20/200\n",
            "96/96 [==============================] - 0s 289us/step - loss: 0.8475 - acc: 0.6875 - val_loss: 0.8431 - val_acc: 0.6667\n",
            "Epoch 21/200\n",
            "96/96 [==============================] - 0s 288us/step - loss: 0.8253 - acc: 0.6875 - val_loss: 0.8195 - val_acc: 0.6667\n",
            "Epoch 22/200\n",
            "96/96 [==============================] - 0s 285us/step - loss: 0.8028 - acc: 0.6875 - val_loss: 0.7985 - val_acc: 0.6667\n",
            "Epoch 23/200\n",
            "96/96 [==============================] - 0s 282us/step - loss: 0.7816 - acc: 0.6875 - val_loss: 0.7745 - val_acc: 0.6667\n",
            "Epoch 24/200\n",
            "96/96 [==============================] - 0s 280us/step - loss: 0.7593 - acc: 0.6875 - val_loss: 0.7508 - val_acc: 0.6667\n",
            "Epoch 25/200\n",
            "96/96 [==============================] - 0s 307us/step - loss: 0.7383 - acc: 0.6875 - val_loss: 0.7298 - val_acc: 0.6667\n",
            "Epoch 26/200\n",
            "96/96 [==============================] - 0s 294us/step - loss: 0.7186 - acc: 0.6875 - val_loss: 0.7066 - val_acc: 0.6667\n",
            "Epoch 27/200\n",
            "96/96 [==============================] - 0s 295us/step - loss: 0.6994 - acc: 0.6875 - val_loss: 0.6899 - val_acc: 0.6667\n",
            "Epoch 28/200\n",
            "96/96 [==============================] - 0s 285us/step - loss: 0.6798 - acc: 0.6875 - val_loss: 0.6670 - val_acc: 0.6667\n",
            "Epoch 29/200\n",
            "96/96 [==============================] - 0s 281us/step - loss: 0.6623 - acc: 0.6875 - val_loss: 0.6502 - val_acc: 0.6667\n",
            "Epoch 30/200\n",
            "96/96 [==============================] - 0s 272us/step - loss: 0.6441 - acc: 0.6875 - val_loss: 0.6307 - val_acc: 0.6667\n",
            "Epoch 31/200\n",
            "96/96 [==============================] - 0s 285us/step - loss: 0.6287 - acc: 0.6875 - val_loss: 0.6149 - val_acc: 0.7083\n",
            "Epoch 32/200\n",
            "96/96 [==============================] - 0s 278us/step - loss: 0.6133 - acc: 0.6875 - val_loss: 0.5960 - val_acc: 0.7083\n",
            "Epoch 33/200\n",
            "96/96 [==============================] - 0s 276us/step - loss: 0.5978 - acc: 0.7083 - val_loss: 0.5808 - val_acc: 0.7083\n",
            "Epoch 34/200\n",
            "96/96 [==============================] - 0s 308us/step - loss: 0.5833 - acc: 0.7083 - val_loss: 0.5660 - val_acc: 0.7083\n",
            "Epoch 35/200\n",
            "96/96 [==============================] - 0s 301us/step - loss: 0.5692 - acc: 0.7292 - val_loss: 0.5492 - val_acc: 0.7083\n",
            "Epoch 36/200\n",
            "96/96 [==============================] - 0s 294us/step - loss: 0.5558 - acc: 0.8125 - val_loss: 0.5340 - val_acc: 0.7917\n",
            "Epoch 37/200\n",
            "96/96 [==============================] - 0s 283us/step - loss: 0.5433 - acc: 0.7708 - val_loss: 0.5245 - val_acc: 0.7083\n",
            "Epoch 38/200\n",
            "96/96 [==============================] - 0s 263us/step - loss: 0.5312 - acc: 0.8333 - val_loss: 0.5095 - val_acc: 0.8333\n",
            "Epoch 39/200\n",
            "96/96 [==============================] - 0s 280us/step - loss: 0.5195 - acc: 0.8542 - val_loss: 0.4988 - val_acc: 0.8333\n",
            "Epoch 40/200\n",
            "96/96 [==============================] - 0s 277us/step - loss: 0.5084 - acc: 0.8854 - val_loss: 0.4848 - val_acc: 0.9167\n",
            "Epoch 41/200\n",
            "96/96 [==============================] - 0s 298us/step - loss: 0.4972 - acc: 0.9167 - val_loss: 0.4750 - val_acc: 0.9167\n",
            "Epoch 42/200\n",
            "96/96 [==============================] - 0s 269us/step - loss: 0.4890 - acc: 0.8750 - val_loss: 0.4669 - val_acc: 0.8333\n",
            "Epoch 43/200\n",
            "96/96 [==============================] - 0s 319us/step - loss: 0.4780 - acc: 0.8750 - val_loss: 0.4578 - val_acc: 0.8333\n",
            "Epoch 44/200\n",
            "96/96 [==============================] - 0s 297us/step - loss: 0.4678 - acc: 0.8958 - val_loss: 0.4467 - val_acc: 0.9167\n",
            "Epoch 45/200\n",
            "96/96 [==============================] - 0s 281us/step - loss: 0.4588 - acc: 0.9271 - val_loss: 0.4373 - val_acc: 0.9167\n",
            "Epoch 46/200\n",
            "96/96 [==============================] - 0s 279us/step - loss: 0.4500 - acc: 0.9688 - val_loss: 0.4286 - val_acc: 0.9167\n",
            "Epoch 47/200\n",
            "96/96 [==============================] - 0s 493us/step - loss: 0.4415 - acc: 0.9375 - val_loss: 0.4215 - val_acc: 0.9167\n",
            "Epoch 48/200\n",
            "96/96 [==============================] - 0s 347us/step - loss: 0.4338 - acc: 0.9167 - val_loss: 0.4137 - val_acc: 0.9167\n",
            "Epoch 49/200\n",
            "96/96 [==============================] - 0s 317us/step - loss: 0.4269 - acc: 0.8958 - val_loss: 0.4086 - val_acc: 0.9167\n",
            "Epoch 50/200\n",
            "96/96 [==============================] - 0s 312us/step - loss: 0.4175 - acc: 0.9271 - val_loss: 0.3980 - val_acc: 0.9583\n",
            "Epoch 51/200\n",
            "96/96 [==============================] - 0s 298us/step - loss: 0.4102 - acc: 0.9792 - val_loss: 0.3902 - val_acc: 0.9583\n",
            "Epoch 52/200\n",
            "96/96 [==============================] - 0s 281us/step - loss: 0.4036 - acc: 0.9792 - val_loss: 0.3846 - val_acc: 0.9583\n",
            "Epoch 53/200\n",
            "96/96 [==============================] - 0s 278us/step - loss: 0.3991 - acc: 0.9792 - val_loss: 0.3753 - val_acc: 0.9583\n",
            "Epoch 54/200\n",
            "96/96 [==============================] - 0s 267us/step - loss: 0.3895 - acc: 0.9792 - val_loss: 0.3736 - val_acc: 0.9583\n",
            "Epoch 55/200\n",
            "96/96 [==============================] - 0s 266us/step - loss: 0.3842 - acc: 0.9583 - val_loss: 0.3690 - val_acc: 0.9583\n",
            "Epoch 56/200\n",
            "96/96 [==============================] - 0s 279us/step - loss: 0.3796 - acc: 0.9792 - val_loss: 0.3585 - val_acc: 0.9583\n",
            "Epoch 57/200\n",
            "96/96 [==============================] - 0s 308us/step - loss: 0.3716 - acc: 0.9792 - val_loss: 0.3578 - val_acc: 0.9583\n",
            "Epoch 58/200\n",
            "96/96 [==============================] - 0s 300us/step - loss: 0.3662 - acc: 0.9792 - val_loss: 0.3508 - val_acc: 0.9583\n",
            "Epoch 59/200\n",
            "96/96 [==============================] - 0s 305us/step - loss: 0.3610 - acc: 0.9792 - val_loss: 0.3443 - val_acc: 0.9583\n",
            "Epoch 60/200\n",
            "96/96 [==============================] - 0s 304us/step - loss: 0.3559 - acc: 0.9792 - val_loss: 0.3437 - val_acc: 0.9583\n",
            "Epoch 61/200\n",
            "96/96 [==============================] - 0s 427us/step - loss: 0.3517 - acc: 0.9792 - val_loss: 0.3347 - val_acc: 0.9583\n",
            "Epoch 62/200\n",
            "96/96 [==============================] - 0s 334us/step - loss: 0.3474 - acc: 0.9792 - val_loss: 0.3308 - val_acc: 0.9583\n",
            "Epoch 63/200\n",
            "96/96 [==============================] - 0s 295us/step - loss: 0.3414 - acc: 0.9792 - val_loss: 0.3263 - val_acc: 0.9583\n",
            "Epoch 64/200\n",
            "96/96 [==============================] - 0s 297us/step - loss: 0.3387 - acc: 0.9792 - val_loss: 0.3222 - val_acc: 0.9583\n",
            "Epoch 65/200\n",
            "96/96 [==============================] - 0s 301us/step - loss: 0.3323 - acc: 0.9792 - val_loss: 0.3198 - val_acc: 0.9583\n",
            "Epoch 66/200\n",
            "96/96 [==============================] - 0s 323us/step - loss: 0.3287 - acc: 0.9792 - val_loss: 0.3189 - val_acc: 0.9583\n",
            "Epoch 67/200\n",
            "96/96 [==============================] - 0s 287us/step - loss: 0.3238 - acc: 0.9792 - val_loss: 0.3098 - val_acc: 0.9583\n",
            "Epoch 68/200\n",
            "96/96 [==============================] - 0s 295us/step - loss: 0.3221 - acc: 0.9792 - val_loss: 0.3092 - val_acc: 0.9583\n",
            "Epoch 69/200\n",
            "96/96 [==============================] - 0s 288us/step - loss: 0.3164 - acc: 0.9792 - val_loss: 0.3047 - val_acc: 0.9583\n",
            "Epoch 70/200\n",
            "96/96 [==============================] - 0s 281us/step - loss: 0.3151 - acc: 0.9792 - val_loss: 0.3040 - val_acc: 0.9583\n",
            "Epoch 71/200\n",
            "96/96 [==============================] - 0s 288us/step - loss: 0.3087 - acc: 0.9792 - val_loss: 0.2960 - val_acc: 0.9583\n",
            "Epoch 72/200\n",
            "96/96 [==============================] - 0s 278us/step - loss: 0.3068 - acc: 0.9792 - val_loss: 0.2922 - val_acc: 0.9583\n",
            "Epoch 73/200\n",
            "96/96 [==============================] - 0s 282us/step - loss: 0.3027 - acc: 0.9792 - val_loss: 0.2911 - val_acc: 0.9583\n",
            "Epoch 74/200\n",
            "96/96 [==============================] - 0s 294us/step - loss: 0.3036 - acc: 0.9792 - val_loss: 0.2945 - val_acc: 0.9583\n",
            "Epoch 75/200\n",
            "96/96 [==============================] - 0s 293us/step - loss: 0.2963 - acc: 0.9792 - val_loss: 0.2861 - val_acc: 0.9583\n",
            "Epoch 76/200\n",
            "96/96 [==============================] - 0s 301us/step - loss: 0.2922 - acc: 0.9792 - val_loss: 0.2856 - val_acc: 0.9583\n",
            "Epoch 77/200\n",
            "96/96 [==============================] - 0s 274us/step - loss: 0.2896 - acc: 0.9792 - val_loss: 0.2844 - val_acc: 0.9583\n",
            "Epoch 78/200\n",
            "96/96 [==============================] - 0s 305us/step - loss: 0.2879 - acc: 0.9792 - val_loss: 0.2818 - val_acc: 0.9583\n",
            "Epoch 79/200\n",
            "96/96 [==============================] - 0s 286us/step - loss: 0.2834 - acc: 0.9792 - val_loss: 0.2745 - val_acc: 0.9583\n",
            "Epoch 80/200\n",
            "96/96 [==============================] - 0s 296us/step - loss: 0.2814 - acc: 0.9792 - val_loss: 0.2717 - val_acc: 0.9583\n",
            "Epoch 81/200\n",
            "96/96 [==============================] - 0s 292us/step - loss: 0.2781 - acc: 0.9792 - val_loss: 0.2699 - val_acc: 0.9583\n",
            "Epoch 82/200\n",
            "96/96 [==============================] - 0s 307us/step - loss: 0.2767 - acc: 0.9792 - val_loss: 0.2711 - val_acc: 0.9583\n",
            "Epoch 83/200\n",
            "96/96 [==============================] - 0s 304us/step - loss: 0.2733 - acc: 0.9792 - val_loss: 0.2669 - val_acc: 0.9583\n",
            "Epoch 84/200\n",
            "96/96 [==============================] - 0s 286us/step - loss: 0.2719 - acc: 0.9792 - val_loss: 0.2668 - val_acc: 0.9583\n",
            "Epoch 85/200\n",
            "96/96 [==============================] - 0s 297us/step - loss: 0.2678 - acc: 0.9792 - val_loss: 0.2593 - val_acc: 0.9583\n",
            "Epoch 86/200\n",
            "96/96 [==============================] - 0s 310us/step - loss: 0.2659 - acc: 0.9792 - val_loss: 0.2546 - val_acc: 0.9583\n",
            "Epoch 87/200\n",
            "96/96 [==============================] - 0s 304us/step - loss: 0.2657 - acc: 0.9792 - val_loss: 0.2562 - val_acc: 0.9583\n",
            "Epoch 88/200\n",
            "96/96 [==============================] - 0s 298us/step - loss: 0.2638 - acc: 0.9792 - val_loss: 0.2509 - val_acc: 0.9583\n",
            "Epoch 89/200\n",
            "96/96 [==============================] - 0s 289us/step - loss: 0.2588 - acc: 0.9792 - val_loss: 0.2530 - val_acc: 0.9583\n",
            "Epoch 90/200\n",
            "96/96 [==============================] - 0s 308us/step - loss: 0.2559 - acc: 0.9792 - val_loss: 0.2479 - val_acc: 0.9583\n",
            "Epoch 91/200\n",
            "96/96 [==============================] - 0s 294us/step - loss: 0.2537 - acc: 0.9792 - val_loss: 0.2496 - val_acc: 0.9583\n",
            "Epoch 92/200\n",
            "96/96 [==============================] - 0s 297us/step - loss: 0.2515 - acc: 0.9792 - val_loss: 0.2487 - val_acc: 0.9583\n",
            "Epoch 93/200\n",
            "96/96 [==============================] - 0s 299us/step - loss: 0.2510 - acc: 0.9792 - val_loss: 0.2479 - val_acc: 0.9583\n",
            "Epoch 94/200\n",
            "96/96 [==============================] - 0s 305us/step - loss: 0.2470 - acc: 0.9792 - val_loss: 0.2397 - val_acc: 0.9583\n",
            "Epoch 95/200\n",
            "96/96 [==============================] - 0s 295us/step - loss: 0.2443 - acc: 0.9792 - val_loss: 0.2383 - val_acc: 0.9583\n",
            "Epoch 96/200\n",
            "96/96 [==============================] - 0s 341us/step - loss: 0.2444 - acc: 0.9792 - val_loss: 0.2394 - val_acc: 0.9583\n",
            "Epoch 97/200\n",
            "96/96 [==============================] - 0s 293us/step - loss: 0.2408 - acc: 0.9792 - val_loss: 0.2354 - val_acc: 0.9583\n",
            "Epoch 98/200\n",
            "96/96 [==============================] - 0s 282us/step - loss: 0.2387 - acc: 0.9792 - val_loss: 0.2307 - val_acc: 0.9583\n",
            "Epoch 99/200\n",
            "96/96 [==============================] - 0s 319us/step - loss: 0.2361 - acc: 0.9792 - val_loss: 0.2306 - val_acc: 0.9583\n",
            "Epoch 100/200\n",
            "96/96 [==============================] - 0s 290us/step - loss: 0.2357 - acc: 0.9792 - val_loss: 0.2323 - val_acc: 0.9583\n",
            "Epoch 101/200\n",
            "96/96 [==============================] - 0s 300us/step - loss: 0.2333 - acc: 0.9792 - val_loss: 0.2283 - val_acc: 0.9583\n",
            "Epoch 102/200\n",
            "96/96 [==============================] - 0s 304us/step - loss: 0.2302 - acc: 0.9792 - val_loss: 0.2258 - val_acc: 0.9583\n",
            "Epoch 103/200\n",
            "96/96 [==============================] - 0s 287us/step - loss: 0.2330 - acc: 0.9688 - val_loss: 0.2200 - val_acc: 0.9583\n",
            "Epoch 104/200\n",
            "96/96 [==============================] - 0s 325us/step - loss: 0.2280 - acc: 0.9792 - val_loss: 0.2236 - val_acc: 0.9583\n",
            "Epoch 105/200\n",
            "96/96 [==============================] - 0s 277us/step - loss: 0.2254 - acc: 0.9792 - val_loss: 0.2197 - val_acc: 0.9583\n",
            "Epoch 106/200\n",
            "96/96 [==============================] - 0s 272us/step - loss: 0.2228 - acc: 0.9792 - val_loss: 0.2192 - val_acc: 0.9583\n",
            "Epoch 107/200\n",
            "96/96 [==============================] - 0s 288us/step - loss: 0.2230 - acc: 0.9792 - val_loss: 0.2207 - val_acc: 0.9583\n",
            "Epoch 108/200\n",
            "96/96 [==============================] - 0s 277us/step - loss: 0.2212 - acc: 0.9792 - val_loss: 0.2180 - val_acc: 0.9583\n",
            "Epoch 109/200\n",
            "96/96 [==============================] - 0s 291us/step - loss: 0.2192 - acc: 0.9792 - val_loss: 0.2144 - val_acc: 0.9583\n",
            "Epoch 110/200\n",
            "96/96 [==============================] - 0s 287us/step - loss: 0.2167 - acc: 0.9792 - val_loss: 0.2124 - val_acc: 0.9583\n",
            "Epoch 111/200\n",
            "96/96 [==============================] - 0s 286us/step - loss: 0.2150 - acc: 0.9792 - val_loss: 0.2089 - val_acc: 0.9583\n",
            "Epoch 112/200\n",
            "96/96 [==============================] - 0s 303us/step - loss: 0.2151 - acc: 0.9792 - val_loss: 0.2106 - val_acc: 0.9583\n",
            "Epoch 113/200\n",
            "96/96 [==============================] - 0s 305us/step - loss: 0.2124 - acc: 0.9792 - val_loss: 0.2077 - val_acc: 0.9583\n",
            "Epoch 114/200\n",
            "96/96 [==============================] - 0s 296us/step - loss: 0.2102 - acc: 0.9792 - val_loss: 0.2063 - val_acc: 0.9583\n",
            "Epoch 115/200\n",
            "96/96 [==============================] - 0s 280us/step - loss: 0.2095 - acc: 0.9792 - val_loss: 0.2071 - val_acc: 0.9583\n",
            "Epoch 116/200\n",
            "96/96 [==============================] - 0s 289us/step - loss: 0.2058 - acc: 0.9792 - val_loss: 0.1996 - val_acc: 0.9583\n",
            "Epoch 117/200\n",
            "96/96 [==============================] - 0s 287us/step - loss: 0.2057 - acc: 0.9792 - val_loss: 0.2000 - val_acc: 0.9583\n",
            "Epoch 118/200\n",
            "96/96 [==============================] - 0s 252us/step - loss: 0.2038 - acc: 0.9792 - val_loss: 0.1982 - val_acc: 0.9583\n",
            "Epoch 119/200\n",
            "96/96 [==============================] - 0s 302us/step - loss: 0.2024 - acc: 0.9792 - val_loss: 0.2017 - val_acc: 0.9583\n",
            "Epoch 120/200\n",
            "96/96 [==============================] - 0s 302us/step - loss: 0.2019 - acc: 0.9792 - val_loss: 0.1976 - val_acc: 0.9583\n",
            "Epoch 121/200\n",
            "96/96 [==============================] - 0s 275us/step - loss: 0.1991 - acc: 0.9792 - val_loss: 0.1995 - val_acc: 0.9583\n",
            "Epoch 122/200\n",
            "96/96 [==============================] - 0s 281us/step - loss: 0.1987 - acc: 0.9792 - val_loss: 0.1963 - val_acc: 0.9583\n",
            "Epoch 123/200\n",
            "96/96 [==============================] - 0s 282us/step - loss: 0.1967 - acc: 0.9792 - val_loss: 0.1949 - val_acc: 0.9583\n",
            "Epoch 124/200\n",
            "96/96 [==============================] - 0s 305us/step - loss: 0.1972 - acc: 0.9792 - val_loss: 0.1999 - val_acc: 0.9583\n",
            "Epoch 125/200\n",
            "96/96 [==============================] - 0s 291us/step - loss: 0.1951 - acc: 0.9792 - val_loss: 0.1946 - val_acc: 0.9583\n",
            "Epoch 126/200\n",
            "96/96 [==============================] - 0s 305us/step - loss: 0.1953 - acc: 0.9792 - val_loss: 0.1844 - val_acc: 0.9583\n",
            "Epoch 127/200\n",
            "96/96 [==============================] - 0s 376us/step - loss: 0.1940 - acc: 0.9688 - val_loss: 0.1897 - val_acc: 0.9583\n",
            "Epoch 128/200\n",
            "96/96 [==============================] - 0s 284us/step - loss: 0.1904 - acc: 0.9792 - val_loss: 0.1834 - val_acc: 0.9583\n",
            "Epoch 129/200\n",
            "96/96 [==============================] - 0s 320us/step - loss: 0.1894 - acc: 0.9688 - val_loss: 0.1843 - val_acc: 0.9583\n",
            "Epoch 130/200\n",
            "96/96 [==============================] - 0s 292us/step - loss: 0.1888 - acc: 0.9792 - val_loss: 0.1834 - val_acc: 0.9583\n",
            "Epoch 131/200\n",
            "96/96 [==============================] - 0s 327us/step - loss: 0.1866 - acc: 0.9792 - val_loss: 0.1845 - val_acc: 0.9583\n",
            "Epoch 132/200\n",
            "96/96 [==============================] - 0s 285us/step - loss: 0.1872 - acc: 0.9792 - val_loss: 0.1835 - val_acc: 0.9583\n",
            "Epoch 133/200\n",
            "96/96 [==============================] - 0s 309us/step - loss: 0.1837 - acc: 0.9792 - val_loss: 0.1780 - val_acc: 0.9583\n",
            "Epoch 134/200\n",
            "96/96 [==============================] - 0s 288us/step - loss: 0.1853 - acc: 0.9688 - val_loss: 0.1747 - val_acc: 0.9583\n",
            "Epoch 135/200\n",
            "96/96 [==============================] - 0s 302us/step - loss: 0.1834 - acc: 0.9792 - val_loss: 0.1833 - val_acc: 0.9583\n",
            "Epoch 136/200\n",
            "96/96 [==============================] - 0s 269us/step - loss: 0.1808 - acc: 0.9792 - val_loss: 0.1793 - val_acc: 0.9583\n",
            "Epoch 137/200\n",
            "96/96 [==============================] - 0s 279us/step - loss: 0.1795 - acc: 0.9792 - val_loss: 0.1786 - val_acc: 0.9583\n",
            "Epoch 138/200\n",
            "96/96 [==============================] - 0s 276us/step - loss: 0.1782 - acc: 0.9792 - val_loss: 0.1770 - val_acc: 0.9583\n",
            "Epoch 139/200\n",
            "96/96 [==============================] - 0s 286us/step - loss: 0.1770 - acc: 0.9792 - val_loss: 0.1756 - val_acc: 0.9583\n",
            "Epoch 140/200\n",
            "96/96 [==============================] - 0s 288us/step - loss: 0.1767 - acc: 0.9792 - val_loss: 0.1724 - val_acc: 0.9583\n",
            "Epoch 141/200\n",
            "96/96 [==============================] - 0s 339us/step - loss: 0.1766 - acc: 0.9792 - val_loss: 0.1721 - val_acc: 0.9583\n",
            "Epoch 142/200\n",
            "96/96 [==============================] - 0s 291us/step - loss: 0.1743 - acc: 0.9688 - val_loss: 0.1679 - val_acc: 0.9583\n",
            "Epoch 143/200\n",
            "96/96 [==============================] - 0s 290us/step - loss: 0.1755 - acc: 0.9792 - val_loss: 0.1721 - val_acc: 0.9583\n",
            "Epoch 144/200\n",
            "96/96 [==============================] - 0s 304us/step - loss: 0.1714 - acc: 0.9792 - val_loss: 0.1694 - val_acc: 0.9583\n",
            "Epoch 145/200\n",
            "96/96 [==============================] - 0s 276us/step - loss: 0.1713 - acc: 0.9792 - val_loss: 0.1661 - val_acc: 0.9583\n",
            "Epoch 146/200\n",
            "96/96 [==============================] - 0s 276us/step - loss: 0.1708 - acc: 0.9792 - val_loss: 0.1684 - val_acc: 0.9583\n",
            "Epoch 147/200\n",
            "96/96 [==============================] - 0s 286us/step - loss: 0.1691 - acc: 0.9792 - val_loss: 0.1639 - val_acc: 0.9583\n",
            "Epoch 148/200\n",
            "96/96 [==============================] - 0s 285us/step - loss: 0.1682 - acc: 0.9792 - val_loss: 0.1644 - val_acc: 0.9583\n",
            "Epoch 149/200\n",
            "96/96 [==============================] - 0s 347us/step - loss: 0.1680 - acc: 0.9792 - val_loss: 0.1664 - val_acc: 0.9583\n",
            "Epoch 150/200\n",
            "96/96 [==============================] - 0s 318us/step - loss: 0.1669 - acc: 0.9792 - val_loss: 0.1631 - val_acc: 0.9583\n",
            "Epoch 151/200\n",
            "96/96 [==============================] - 0s 290us/step - loss: 0.1652 - acc: 0.9792 - val_loss: 0.1618 - val_acc: 0.9583\n",
            "Epoch 152/200\n",
            "96/96 [==============================] - 0s 292us/step - loss: 0.1658 - acc: 0.9792 - val_loss: 0.1628 - val_acc: 0.9583\n",
            "Epoch 153/200\n",
            "96/96 [==============================] - 0s 292us/step - loss: 0.1643 - acc: 0.9792 - val_loss: 0.1622 - val_acc: 0.9583\n",
            "Epoch 154/200\n",
            "96/96 [==============================] - 0s 295us/step - loss: 0.1621 - acc: 0.9792 - val_loss: 0.1585 - val_acc: 0.9583\n",
            "Epoch 155/200\n",
            "96/96 [==============================] - 0s 286us/step - loss: 0.1617 - acc: 0.9792 - val_loss: 0.1556 - val_acc: 0.9583\n",
            "Epoch 156/200\n",
            "96/96 [==============================] - 0s 282us/step - loss: 0.1614 - acc: 0.9792 - val_loss: 0.1557 - val_acc: 0.9583\n",
            "Epoch 157/200\n",
            "96/96 [==============================] - 0s 283us/step - loss: 0.1604 - acc: 0.9792 - val_loss: 0.1550 - val_acc: 0.9583\n",
            "Epoch 158/200\n",
            "96/96 [==============================] - 0s 287us/step - loss: 0.1602 - acc: 0.9688 - val_loss: 0.1531 - val_acc: 0.9583\n",
            "Epoch 159/200\n",
            "96/96 [==============================] - 0s 289us/step - loss: 0.1592 - acc: 0.9792 - val_loss: 0.1548 - val_acc: 0.9583\n",
            "Epoch 160/200\n",
            "96/96 [==============================] - 0s 305us/step - loss: 0.1576 - acc: 0.9792 - val_loss: 0.1553 - val_acc: 0.9583\n",
            "Epoch 161/200\n",
            "96/96 [==============================] - 0s 295us/step - loss: 0.1565 - acc: 0.9792 - val_loss: 0.1547 - val_acc: 0.9583\n",
            "Epoch 162/200\n",
            "96/96 [==============================] - 0s 293us/step - loss: 0.1560 - acc: 0.9792 - val_loss: 0.1551 - val_acc: 0.9583\n",
            "Epoch 163/200\n",
            "96/96 [==============================] - 0s 278us/step - loss: 0.1552 - acc: 0.9792 - val_loss: 0.1561 - val_acc: 0.9583\n",
            "Epoch 164/200\n",
            "96/96 [==============================] - 0s 294us/step - loss: 0.1564 - acc: 0.9792 - val_loss: 0.1558 - val_acc: 0.9583\n",
            "Epoch 165/200\n",
            "96/96 [==============================] - 0s 280us/step - loss: 0.1530 - acc: 0.9792 - val_loss: 0.1495 - val_acc: 0.9583\n",
            "Epoch 166/200\n",
            "96/96 [==============================] - 0s 293us/step - loss: 0.1534 - acc: 0.9792 - val_loss: 0.1499 - val_acc: 0.9583\n",
            "Epoch 167/200\n",
            "96/96 [==============================] - 0s 311us/step - loss: 0.1515 - acc: 0.9792 - val_loss: 0.1499 - val_acc: 0.9583\n",
            "Epoch 168/200\n",
            "96/96 [==============================] - 0s 275us/step - loss: 0.1515 - acc: 0.9792 - val_loss: 0.1498 - val_acc: 0.9583\n",
            "Epoch 169/200\n",
            "96/96 [==============================] - 0s 328us/step - loss: 0.1509 - acc: 0.9792 - val_loss: 0.1462 - val_acc: 0.9583\n",
            "Epoch 170/200\n",
            "96/96 [==============================] - 0s 345us/step - loss: 0.1501 - acc: 0.9792 - val_loss: 0.1463 - val_acc: 0.9583\n",
            "Epoch 171/200\n",
            "96/96 [==============================] - 0s 295us/step - loss: 0.1497 - acc: 0.9688 - val_loss: 0.1433 - val_acc: 0.9583\n",
            "Epoch 172/200\n",
            "96/96 [==============================] - 0s 278us/step - loss: 0.1493 - acc: 0.9688 - val_loss: 0.1417 - val_acc: 0.9583\n",
            "Epoch 173/200\n",
            "96/96 [==============================] - 0s 298us/step - loss: 0.1480 - acc: 0.9792 - val_loss: 0.1425 - val_acc: 0.9583\n",
            "Epoch 174/200\n",
            "96/96 [==============================] - 0s 288us/step - loss: 0.1461 - acc: 0.9792 - val_loss: 0.1454 - val_acc: 0.9583\n",
            "Epoch 175/200\n",
            "96/96 [==============================] - 0s 284us/step - loss: 0.1459 - acc: 0.9792 - val_loss: 0.1482 - val_acc: 0.9583\n",
            "Epoch 176/200\n",
            "96/96 [==============================] - 0s 299us/step - loss: 0.1460 - acc: 0.9792 - val_loss: 0.1446 - val_acc: 0.9583\n",
            "Epoch 177/200\n",
            "96/96 [==============================] - 0s 316us/step - loss: 0.1452 - acc: 0.9792 - val_loss: 0.1430 - val_acc: 0.9583\n",
            "Epoch 178/200\n",
            "96/96 [==============================] - 0s 301us/step - loss: 0.1441 - acc: 0.9792 - val_loss: 0.1403 - val_acc: 0.9583\n",
            "Epoch 179/200\n",
            "96/96 [==============================] - 0s 275us/step - loss: 0.1461 - acc: 0.9688 - val_loss: 0.1371 - val_acc: 0.9583\n",
            "Epoch 180/200\n",
            "96/96 [==============================] - 0s 287us/step - loss: 0.1428 - acc: 0.9792 - val_loss: 0.1433 - val_acc: 0.9583\n",
            "Epoch 181/200\n",
            "96/96 [==============================] - 0s 281us/step - loss: 0.1421 - acc: 0.9792 - val_loss: 0.1395 - val_acc: 0.9583\n",
            "Epoch 182/200\n",
            "96/96 [==============================] - 0s 290us/step - loss: 0.1426 - acc: 0.9792 - val_loss: 0.1378 - val_acc: 0.9583\n",
            "Epoch 183/200\n",
            "96/96 [==============================] - 0s 281us/step - loss: 0.1412 - acc: 0.9792 - val_loss: 0.1380 - val_acc: 0.9583\n",
            "Epoch 184/200\n",
            "96/96 [==============================] - 0s 300us/step - loss: 0.1405 - acc: 0.9792 - val_loss: 0.1423 - val_acc: 0.9583\n",
            "Epoch 185/200\n",
            "96/96 [==============================] - 0s 299us/step - loss: 0.1422 - acc: 0.9792 - val_loss: 0.1365 - val_acc: 0.9583\n",
            "Epoch 186/200\n",
            "96/96 [==============================] - 0s 288us/step - loss: 0.1392 - acc: 0.9792 - val_loss: 0.1398 - val_acc: 0.9583\n",
            "Epoch 187/200\n",
            "96/96 [==============================] - 0s 281us/step - loss: 0.1384 - acc: 0.9792 - val_loss: 0.1356 - val_acc: 0.9583\n",
            "Epoch 188/200\n",
            "96/96 [==============================] - 0s 293us/step - loss: 0.1381 - acc: 0.9792 - val_loss: 0.1347 - val_acc: 0.9583\n",
            "Epoch 189/200\n",
            "96/96 [==============================] - 0s 281us/step - loss: 0.1375 - acc: 0.9792 - val_loss: 0.1359 - val_acc: 0.9583\n",
            "Epoch 190/200\n",
            "96/96 [==============================] - 0s 281us/step - loss: 0.1366 - acc: 0.9792 - val_loss: 0.1344 - val_acc: 0.9583\n",
            "Epoch 191/200\n",
            "96/96 [==============================] - 0s 279us/step - loss: 0.1364 - acc: 0.9792 - val_loss: 0.1325 - val_acc: 0.9583\n",
            "Epoch 192/200\n",
            "96/96 [==============================] - 0s 303us/step - loss: 0.1350 - acc: 0.9792 - val_loss: 0.1313 - val_acc: 0.9583\n",
            "Epoch 193/200\n",
            "96/96 [==============================] - 0s 279us/step - loss: 0.1351 - acc: 0.9792 - val_loss: 0.1305 - val_acc: 0.9583\n",
            "Epoch 194/200\n",
            "96/96 [==============================] - 0s 299us/step - loss: 0.1365 - acc: 0.9688 - val_loss: 0.1293 - val_acc: 0.9583\n",
            "Epoch 195/200\n",
            "96/96 [==============================] - 0s 273us/step - loss: 0.1344 - acc: 0.9792 - val_loss: 0.1310 - val_acc: 0.9583\n",
            "Epoch 196/200\n",
            "96/96 [==============================] - 0s 285us/step - loss: 0.1339 - acc: 0.9792 - val_loss: 0.1322 - val_acc: 0.9583\n",
            "Epoch 197/200\n",
            "96/96 [==============================] - 0s 281us/step - loss: 0.1343 - acc: 0.9792 - val_loss: 0.1306 - val_acc: 0.9583\n",
            "Epoch 198/200\n",
            "96/96 [==============================] - 0s 290us/step - loss: 0.1327 - acc: 0.9792 - val_loss: 0.1273 - val_acc: 0.9583\n",
            "Epoch 199/200\n",
            "96/96 [==============================] - 0s 294us/step - loss: 0.1315 - acc: 0.9792 - val_loss: 0.1274 - val_acc: 0.9583\n",
            "Epoch 200/200\n",
            "96/96 [==============================] - 0s 283us/step - loss: 0.1322 - acc: 0.9792 - val_loss: 0.1253 - val_acc: 0.9583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHreB--UHEuB",
        "colab_type": "text"
      },
      "source": [
        "### Examining the plot\n",
        "\n",
        "In this model loss plot we can see that the model's loss gradually drops at it reaches 200 epochs and that our training and validation data loss do not deviate from each other meaning there is no overfitting present.\n",
        "\n",
        "We can also see that the accuracy of the training and validation data sets increase in accuracy in a similar fashion meaning which also indicate that there is no significant overfitting in our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwYPK696zTqu",
        "colab_type": "code",
        "outputId": "b9d4832c-6b8e-48ed-b802-6d0164aac127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "def plot_acc_loss(history):\n",
        "    f, (ax1, ax2) = plt.subplots(2,1, figsize=(10,10))\n",
        "    \n",
        "    # Summarize history of accuracy\n",
        "    ax1.plot(history.history['loss'])\n",
        "    ax1.plot(history.history['val_loss'])\n",
        "    ax1.set_title('model loss')\n",
        "    ax1.legend(['train', 'val'], loc='upper left')\n",
        "    \n",
        "    # Summarize history of accuracy\n",
        "    ax2.plot(history.history['acc'])\n",
        "    ax2.plot(history.history['val_acc'])\n",
        "    ax2.set_title('model accuracy')\n",
        "    ax2.set_xlabel('epoch')\n",
        "    ax2.legend(['train', 'val'], loc='upper left')\n",
        "    \n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "    \n",
        "plot_acc_loss(history)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAJcCAYAAAA7Pup5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw2wWtw5zTqy",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the model\n",
        "Now that we have trained our model, we can evaluate it's predictive performance on our test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQtZLcOdzTq0",
        "colab_type": "code",
        "outputId": "4e2bd93b-cd86-4246-a968-383bc76b58e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"\\n\\n{0}: {1:.2f}%\".format(model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r30/30 [==============================] - 0s 108us/step\n",
            "\n",
            "\n",
            "acc: 96.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR2ihtRTBQ9R",
        "colab_type": "text"
      },
      "source": [
        "Our accuracy across our test set is 96.67% meaning that if we were to use this model on new data from the same Iris dataset, it would correctly classify the Iris species 96.67% of the time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnnHSuWHBmIT",
        "colab_type": "text"
      },
      "source": [
        "### The Confusion Matrix\n",
        "\n",
        "A confusion matrix is useful for describing the performance of a classification model. Instead of seeing generalised accuracy, we're able to look how each individual classification performs. \n",
        "\n",
        "In this case we can see that a sample of Versicolor was incorrectly classified as Virginica which we could try to improve in the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEnF0buSzTq4",
        "colab_type": "code",
        "outputId": "d33d8ac2-faaa-4b00-ae2f-d80b99dff277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "def draw_confusion_matrix(true, pred, labels,str_labels):\n",
        "  \"\"\"\n",
        "  Drawing confusion matrix\n",
        "  \"\"\"\n",
        "  cm = metrics.confusion_matrix(true, pred, labels)\n",
        "  ax = plt.subplot()\n",
        "  sns.heatmap(cm, annot=True, ax=ax)\n",
        "  ax.set_xticklabels(str_labels+labels)\n",
        "  ax.set_yticklabels(str_labels+labels)\n",
        "  ax.set_xlabel(\"Predicted Classification\")\n",
        "  ax.set_ylabel(\"True Classification\")\n",
        "  plt.show()\n",
        "  return cm\n",
        "\n",
        "y_pred = model.predict_classes(X_test)\n",
        "y_test_encoded = [np.argmax(i) for i in y_test] # Reverse one hot encoded to label encoded\n",
        "matrix = draw_confusion_matrix(y_test_encoded, y_pred, [0,1,2], ['Setosa', 'Versicolor', 'Virginica']) #[setosa,versicolor, virginica]\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEKCAYAAAA/2c+EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNjMrKFfQZmYF5QrazKyg/j87nLaA4JTYlQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhOFX6LUzTrD",
        "colab_type": "code",
        "outputId": "0af87a5b-f899-47a5-f228-95e258fb263c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"acc: {0:.2f}%\".format(np.sum(np.diag(matrix))*100/np.sum(matrix)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 96.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHe0CcqcsO1Q",
        "colab_type": "text"
      },
      "source": [
        "# Key Points\n",
        "\n",
        "\n",
        "*   Created a neural network to solve multiclass classification problem\n",
        "*   \"One hot encoded\" to categorise string target variable\n",
        "*  Different loss functions for different classfication problems\n",
        "* Confusion matrix and model improvements\n",
        "\n",
        "\n"
      ]
    }
  ]
}